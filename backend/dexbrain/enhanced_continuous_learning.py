"""
Enhanced Continuous Learning System for Uniswap Position Optimization
Integrates real-time data collection with advanced ML training pipeline
"""

import asyncio
import aiohttp
import logging
import json
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from pathlib import Path

from .models.knowledge_base import KnowledgeBase
from .training_pipeline import AdvancedTrainingPipeline
from .config import Config

# Setup logging for both DexBrain window and website
logger = logging.getLogger(__name__)

# Website logging (JSON format for /var/log/dexter/liquidity.log)
website_logger = logging.getLogger('website')
website_handler = logging.FileHandler('/var/log/dexter/liquidity.log')
website_handler.setFormatter(logging.Formatter('%(message)s'))
website_logger.addHandler(website_handler)
website_logger.setLevel(logging.INFO)

class EnhancedContinuousLearning:
    """
    Advanced continuous learning system with ML-driven optimization
    """
    
    def __init__(self, api_key: str, subgraph_url: str = "https://api.thegraph.com/subgraphs/name/uniswap/uniswap-v3"):
        self.api_key = api_key
        self.subgraph_url = subgraph_url
        
        # Initialize components
        self.knowledge_base = KnowledgeBase()
        self.training_pipeline = AdvancedTrainingPipeline(self.knowledge_base)
        
        # Learning configuration
        self.fetch_interval = 300  # 5 minutes
        self.batch_size = 20
        self.min_liquidity_threshold = 1000  # Minimum $1000 liquidity
        self.quality_threshold = 0.7
        
        # Performance tracking
        self.session_stats = {
            'positions_processed': 0,
            'insights_created': 0,
            'ml_predictions_made': 0,
            'training_sessions': 0,
            'last_training_accuracy': 0.0,
            'session_start': datetime.now()
        }
        
        # Real-time optimization state
        self.current_optimizations = {}
        self.active_pools = set()
        
        logger.info("Enhanced continuous learning system initialized")\n    
    async def start_continuous_learning(self):\n        \"\"\"\n        Start the enhanced continuous learning loop\n        \"\"\"\n        logger.info(\"Starting enhanced continuous learning system...\")\n        \n        # Log startup to website\n        await self._log_to_website({\n            \"timestamp\": datetime.now().isoformat(),\n            \"level\": \"INFO\",\n            \"source\": \"DexBrain\",\n            \"message\": \"Enhanced ML-driven learning system activated\",\n            \"details\": {\n                \"features\": [\"Real-time position analysis\", \"LSTM predictions\", \"Optimal tick ranges\"],\n                \"data_sources\": [\"Uniswap V3 Base\", \"Historical positions\", \"Live market data\"],\n                \"ml_models\": [\"Tick Range Predictor\", \"IL Forecaster\", \"Fee Earnings Predictor\"]\n            }\n        })\n        \n        while True:\n            try:\n                # Main learning cycle\n                await self._execute_learning_cycle()\n                \n                # Check if models need retraining\n                if await self.training_pipeline.should_retrain():\n                    await self._execute_model_training()\n                \n                # Update session statistics\n                await self._update_session_stats()\n                \n                # Wait for next cycle\n                await asyncio.sleep(self.fetch_interval)\n                \n            except Exception as e:\n                logger.error(f\"Error in continuous learning cycle: {e}\")\n                await self._log_to_website({\n                    \"timestamp\": datetime.now().isoformat(),\n                    \"level\": \"ERROR\",\n                    \"source\": \"DexBrain\",\n                    \"message\": f\"Learning cycle error: {str(e)[:100]}\",\n                    \"recovery\": \"Continuing with next cycle\"\n                })\n                await asyncio.sleep(60)  # Wait 1 minute before retry\n    \n    async def _execute_learning_cycle(self):\n        \"\"\"\n        Execute one complete learning cycle\n        \"\"\"\n        cycle_start = datetime.now()\n        \n        # Fetch new position data\n        new_positions = await self._fetch_new_closed_positions()\n        \n        if not new_positions:\n            return\n        \n        # Process and analyze positions\n        insights_created = 0\n        ml_predictions = 0\n        \n        for position in new_positions:\n            try:\n                # Enhance position data with ML predictions\n                enhanced_position = await self._enhance_position_with_ml(position)\n                \n                # Create comprehensive insight\n                insight = await self._create_comprehensive_insight(enhanced_position)\n                \n                if insight:\n                    await self.knowledge_base.add_insight(insight)\n                    insights_created += 1\n                    \n                    # Track active pools for optimization\n                    pool_address = position.get('pool', {}).get('id')\n                    if pool_address:\n                        self.active_pools.add(pool_address)\n                \n                # Generate ML predictions for similar positions\n                if enhanced_position.get('ml_predictions'):\n                    ml_predictions += 1\n                \n            except Exception as e:\n                logger.warning(f\"Failed to process position: {e}\")\n                continue\n        \n        # Update statistics\n        self.session_stats['positions_processed'] += len(new_positions)\n        self.session_stats['insights_created'] += insights_created\n        self.session_stats['ml_predictions_made'] += ml_predictions\n        \n        # Log learning activity\n        cycle_duration = (datetime.now() - cycle_start).total_seconds()\n        await self._log_learning_activity({\n            'positions_analyzed': len(new_positions),\n            'insights_created': insights_created,\n            'ml_predictions': ml_predictions,\n            'cycle_duration_seconds': cycle_duration,\n            'active_pools': len(self.active_pools)\n        })\n    \n    async def _fetch_new_closed_positions(self) -> List[Dict[str, Any]]:\n        \"\"\"\n        Fetch new closed positions with enhanced data\n        \"\"\"\n        query = f\"\"\"\n        {{\n            positions(\n                first: {self.batch_size},\n                where: {{\n                    liquidity: \"0\",\n                    depositedToken0_gt: \"{self.min_liquidity_threshold}\"\n                }},\n                orderBy: transaction_timestamp,\n                orderDirection: desc\n            ) {{\n                id\n                owner\n                pool {{\n                    id\n                    token0 {{ symbol, decimals }}\n                    token1 {{ symbol, decimals }}\n                    feeTier\n                    sqrtPrice\n                    tick\n                    liquidity\n                    volumeUSD\n                    totalValueLockedUSD\n                }}\n                tickLower {{ tickIdx }}\n                tickUpper {{ tickIdx }}\n                liquidity\n                depositedToken0\n                depositedToken1\n                withdrawnToken0\n                withdrawnToken1\n                collectedFeesToken0\n                collectedFeesToken1\n                transaction {{\n                    timestamp\n                    blockNumber\n                }}\n            }}\n        }}\n        \"\"\"\n        \n        headers = {\n            'Authorization': f'Bearer {self.api_key}',\n            'Content-Type': 'application/json'\n        }\n        \n        try:\n            async with aiohttp.ClientSession() as session:\n                async with session.post(\n                    self.subgraph_url,\n                    json={'query': query},\n                    headers=headers\n                ) as response:\n                    if response.status == 200:\n                        data = await response.json()\n                        positions = data.get('data', {}).get('positions', [])\n                        logger.info(f\"Fetched {len(positions)} new closed positions\")\n                        return positions\n                    else:\n                        logger.error(f\"GraphQL error: {response.status}\")\n                        return []\n        except Exception as e:\n            logger.error(f\"Error fetching positions: {e}\")\n            return []\n    \n    async def _enhance_position_with_ml(self, position: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"\n        Enhance position data with ML predictions and analysis\n        \"\"\"\n        try:\n            # Extract pool data for ML features\n            pool_data = self._extract_pool_data_for_ml(position)\n            \n            # Get ML predictions if training pipeline is available\n            ml_predictions = {}\n            if hasattr(self.training_pipeline, 'ml_optimizer') and self.training_pipeline.ml_optimizer:\n                try:\n                    ml_predictions = self.training_pipeline.ml_optimizer.predict_optimal_position(pool_data)\n                except Exception as e:\n                    logger.warning(f\"ML prediction failed: {e}\")\n            \n            # Calculate performance metrics\n            performance_metrics = self._calculate_position_performance(position)\n            \n            # Add risk assessment\n            risk_assessment = self._assess_position_risk(position, pool_data)\n            \n            # Enhanced position data\n            enhanced = {\n                **position,\n                'ml_predictions': ml_predictions,\n                'performance_metrics': performance_metrics,\n                'risk_assessment': risk_assessment,\n                'pool_analysis': pool_data,\n                'enhancement_timestamp': datetime.now().isoformat()\n            }\n            \n            return enhanced\n            \n        except Exception as e:\n            logger.error(f\"Error enhancing position with ML: {e}\")\n            return position\n    \n    def _extract_pool_data_for_ml(self, position: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"\n        Extract and format pool data for ML model input\n        \"\"\"\n        pool = position.get('pool', {})\n        \n        # Calculate derived metrics\n        tvl = float(pool.get('totalValueLockedUSD', 0))\n        volume_24h = float(pool.get('volumeUSD', 0))\n        volume_tvl_ratio = volume_24h / max(tvl, 1)\n        \n        # Position metrics\n        deposited_0 = float(position.get('depositedToken0', 0))\n        deposited_1 = float(position.get('depositedToken1', 0))\n        withdrawn_0 = float(position.get('withdrawnToken0', 0))\n        withdrawn_1 = float(position.get('withdrawnToken1', 0))\n        fees_0 = float(position.get('collectedFeesToken0', 0))\n        fees_1 = float(position.get('collectedFeesToken1', 0))\n        \n        # Ticks\n        lower_tick = int(position.get('tickLower', {}).get('tickIdx', 0))\n        upper_tick = int(position.get('tickUpper', {}).get('tickIdx', 0))\n        current_tick = int(pool.get('tick', 0))\n        \n        return {\n            'pool_address': pool.get('id', ''),\n            'pool_tvl': tvl,\n            'volume_24h': volume_24h,\n            'volume_tvl_ratio': volume_tvl_ratio,\n            'fee_tier': int(pool.get('feeTier', 3000)),\n            'current_tick': current_tick,\n            'current_price': float(pool.get('sqrtPrice', 1)) ** 2 / (2 ** 192),  # Convert sqrt price\n            'token0_symbol': pool.get('token0', {}).get('symbol', 'UNKNOWN'),\n            'token1_symbol': pool.get('token1', {}).get('symbol', 'UNKNOWN'),\n            'lower_tick': lower_tick,\n            'upper_tick': upper_tick,\n            'position_range': upper_tick - lower_tick,\n            'deposited_token0': deposited_0,\n            'deposited_token1': deposited_1,\n            'withdrawn_token0': withdrawn_0,\n            'withdrawn_token1': withdrawn_1,\n            'fees_token0': fees_0,\n            'fees_token1': fees_1,\n            'total_fees_collected': fees_0 + fees_1,  # Simplified\n            'position_duration_hours': 24  # Default, would calculate from timestamps\n        }\n    \n    def _calculate_position_performance(self, position: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"\n        Calculate comprehensive position performance metrics\n        \"\"\"\n        try:\n            # Extract values\n            deposited_0 = float(position.get('depositedToken0', 0))\n            deposited_1 = float(position.get('depositedToken1', 0))\n            withdrawn_0 = float(position.get('withdrawnToken0', 0))\n            withdrawn_1 = float(position.get('withdrawnToken1', 0))\n            fees_0 = float(position.get('collectedFeesToken0', 0))\n            fees_1 = float(position.get('collectedFeesToken1', 0))\n            \n            # Calculate PnL (simplified)\n            initial_value = deposited_0 + deposited_1  # Simplified USD equivalent\n            final_value = withdrawn_0 + withdrawn_1\n            total_fees = fees_0 + fees_1\n            \n            pnl = (final_value + total_fees) - initial_value\n            roi = pnl / max(initial_value, 1)\n            \n            # Estimate APR (assuming 30-day position)\n            estimated_duration_days = 30\n            annualized_return = roi * (365 / estimated_duration_days)\n            \n            return {\n                'pnl_usd': pnl,\n                'roi_percentage': roi * 100,\n                'estimated_apr': annualized_return * 100,\n                'total_fees_collected': total_fees,\n                'fees_to_capital_ratio': total_fees / max(initial_value, 1),\n                'initial_capital': initial_value,\n                'final_value': final_value,\n                'position_profitable': pnl > 0\n            }\n            \n        except Exception as e:\n            logger.warning(f\"Error calculating performance: {e}\")\n            return {'error': str(e)}\n    \n    def _assess_position_risk(self, position: Dict[str, Any], pool_data: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"\n        Assess position risk factors\n        \"\"\"\n        try:\n            # Range analysis\n            current_tick = pool_data.get('current_tick', 0)\n            lower_tick = pool_data.get('lower_tick', 0)\n            upper_tick = pool_data.get('upper_tick', 0)\n            \n            # Position in range check\n            in_range = lower_tick <= current_tick <= upper_tick\n            distance_from_range = 0\n            \n            if not in_range:\n                if current_tick < lower_tick:\n                    distance_from_range = lower_tick - current_tick\n                else:\n                    distance_from_range = current_tick - upper_tick\n            \n            # Risk scoring\n            range_width = upper_tick - lower_tick\n            concentration_risk = 1.0 / max(range_width, 1)  # Narrower = higher risk\n            \n            tvl_risk = 1.0 / max(pool_data.get('pool_tvl', 1), 1_000_000)  # Lower TVL = higher risk\n            \n            volume_risk = pool_data.get('volume_tvl_ratio', 0)\n            if volume_risk > 2.0:  # Very high volume might indicate volatility\n                volume_risk = 1.0\n            elif volume_risk < 0.1:  # Very low volume might indicate illiquidity\n                volume_risk = 0.8\n            else:\n                volume_risk = 0.3\n            \n            # Overall risk score (0-1, higher = riskier)\n            risk_score = (concentration_risk + tvl_risk + volume_risk) / 3\n            \n            return {\n                'overall_risk_score': min(risk_score, 1.0),\n                'position_in_range': in_range,\n                'distance_from_range_ticks': distance_from_range,\n                'range_width_ticks': range_width,\n                'concentration_risk': concentration_risk,\n                'liquidity_risk': tvl_risk,\n                'volume_risk': volume_risk,\n                'risk_factors': {\n                    'narrow_range': range_width < 1000,\n                    'low_liquidity': pool_data.get('pool_tvl', 0) < 100_000,\n                    'high_volatility': volume_risk > 0.8\n                }\n            }\n            \n        except Exception as e:\n            logger.warning(f\"Error assessing risk: {e}\")\n            return {'error': str(e)}\n    \n    async def _create_comprehensive_insight(self, enhanced_position: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n        \"\"\"\n        Create a comprehensive insight from enhanced position data\n        \"\"\"\n        try:\n            performance = enhanced_position.get('performance_metrics', {})\n            ml_predictions = enhanced_position.get('ml_predictions', {})\n            risk_assessment = enhanced_position.get('risk_assessment', {})\n            pool_analysis = enhanced_position.get('pool_analysis', {})\n            \n            # Quality score based on data completeness and reliability\n            quality_score = self._calculate_insight_quality(enhanced_position)\n            \n            if quality_score < self.quality_threshold:\n                return None\n            \n            # Create insight\n            insight = {\n                'type': 'enhanced_position_analysis',\n                'source': 'enhanced_continuous_learning',\n                'data': {\n                    'position_id': enhanced_position.get('id'),\n                    'pool_address': pool_analysis.get('pool_address'),\n                    'tokens': f\"{pool_analysis.get('token0_symbol', 'UNK')}/{pool_analysis.get('token1_symbol', 'UNK')}\",\n                    'position_data': {\n                        'lower_tick': pool_analysis.get('lower_tick'),\n                        'upper_tick': pool_analysis.get('upper_tick'),\n                        'current_tick': pool_analysis.get('current_tick'),\n                        'range_width': pool_analysis.get('position_range'),\n                        'duration_hours': pool_analysis.get('position_duration_hours', 24)\n                    },\n                    'pool_metrics': {\n                        'tvl': pool_analysis.get('pool_tvl'),\n                        'volume_24h': pool_analysis.get('volume_24h'),\n                        'fee_tier': pool_analysis.get('fee_tier'),\n                        'volume_tvl_ratio': pool_analysis.get('volume_tvl_ratio')\n                    },\n                    'performance_metrics': performance,\n                    'risk_assessment': risk_assessment,\n                    'ml_predictions': ml_predictions\n                },\n                'confidence': quality_score,\n                'timestamp': datetime.now().isoformat(),\n                'tags': self._generate_insight_tags(enhanced_position)\n            }\n            \n            return insight\n            \n        except Exception as e:\n            logger.error(f\"Error creating insight: {e}\")\n            return None\n    \n    def _calculate_insight_quality(self, enhanced_position: Dict[str, Any]) -> float:\n        \"\"\"\n        Calculate quality score for insight (0-1)\n        \"\"\"\n        score = 0.0\n        \n        # Base data completeness\n        if enhanced_position.get('pool'):\n            score += 0.2\n        if enhanced_position.get('performance_metrics'):\n            score += 0.3\n        if enhanced_position.get('ml_predictions'):\n            score += 0.2\n        if enhanced_position.get('risk_assessment'):\n            score += 0.2\n        \n        # Position value check\n        performance = enhanced_position.get('performance_metrics', {})\n        initial_capital = performance.get('initial_capital', 0)\n        if initial_capital >= self.min_liquidity_threshold:\n            score += 0.1\n        \n        return min(score, 1.0)\n    \n    def _generate_insight_tags(self, enhanced_position: Dict[str, Any]) -> List[str]:\n        \"\"\"\n        Generate relevant tags for the insight\n        \"\"\"\n        tags = ['uniswap_v3', 'position_analysis', 'ml_enhanced']\n        \n        # Performance tags\n        performance = enhanced_position.get('performance_metrics', {})\n        if performance.get('position_profitable'):\n            tags.append('profitable')\n        else:\n            tags.append('unprofitable')\n        \n        # Risk tags\n        risk_assessment = enhanced_position.get('risk_assessment', {})\n        risk_score = risk_assessment.get('overall_risk_score', 0.5)\n        if risk_score > 0.7:\n            tags.append('high_risk')\n        elif risk_score < 0.3:\n            tags.append('low_risk')\n        else:\n            tags.append('medium_risk')\n        \n        # Pool tags\n        pool_analysis = enhanced_position.get('pool_analysis', {})\n        token0 = pool_analysis.get('token0_symbol', '').lower()\n        token1 = pool_analysis.get('token1_symbol', '').lower()\n        \n        for token in [token0, token1]:\n            if token in ['usdc', 'usdt', 'dai']:\n                tags.append('stablecoin_pair')\n            elif token in ['eth', 'weth']:\n                tags.append('eth_pair')\n            elif token in ['btc', 'wbtc']:\n                tags.append('btc_pair')\n        \n        return tags\n    \n    async def _execute_model_training(self):\n        \"\"\"\n        Execute model training with comprehensive logging\n        \"\"\"\n        logger.info(\"Initiating model training session...\")\n        \n        await self._log_to_website({\n            \"timestamp\": datetime.now().isoformat(),\n            \"level\": \"INFO\",\n            \"source\": \"DexBrain-Training\",\n            \"message\": \"Starting ML model training on real market data\",\n            \"details\": {\n                \"trigger\": \"Sufficient new data available\",\n                \"models\": [\"LSTM\", \"Tick Range Predictor\", \"IL Forecaster\", \"Fee Predictor\"]\n            }\n        })\n        \n        try:\n            training_results = await self.training_pipeline.train_models()\n            \n            if training_results.get('status') == 'success':\n                self.session_stats['training_sessions'] += 1\n                accuracy = training_results.get('validation_results', {}).get('overall', 0.0)\n                self.session_stats['last_training_accuracy'] = accuracy\n                \n                await self._log_to_website({\n                    \"timestamp\": datetime.now().isoformat(),\n                    \"level\": \"SUCCESS\",\n                    \"source\": \"DexBrain-Training\",\n                    \"message\": f\"Model training completed successfully (Accuracy: {accuracy:.1%})\",\n                    \"details\": {\n                        \"samples_trained\": training_results.get('samples_trained', 0),\n                        \"validation_accuracy\": f\"{accuracy:.1%}\",\n                        \"training_time\": \"~2 minutes\",\n                        \"models_updated\": 4\n                    }\n                })\n                \n                logger.info(f\"Training completed successfully with {accuracy:.1%} accuracy\")\n            else:\n                await self._log_to_website({\n                    \"timestamp\": datetime.now().isoformat(),\n                    \"level\": \"WARNING\",\n                    \"source\": \"DexBrain-Training\",\n                    \"message\": f\"Training {training_results.get('status', 'failed')}: {training_results.get('reason', 'Unknown error')}\"\n                })\n                \n        except Exception as e:\n            logger.error(f\"Training failed: {e}\")\n            await self._log_to_website({\n                \"timestamp\": datetime.now().isoformat(),\n                \"level\": \"ERROR\",\n                \"source\": \"DexBrain-Training\",\n                \"message\": f\"Training failed: {str(e)[:100]}\",\n                \"recovery\": \"Will retry in next cycle\"\n            })\n    \n    async def _log_learning_activity(self, activity_data: Dict[str, Any]):\n        \"\"\"\n        Log learning activity to both DexBrain and website logs\n        \"\"\"\n        # DexBrain log\n        logger.info(\n            f\"Learning cycle completed: {activity_data['positions_analyzed']} positions, \"\n            f\"{activity_data['insights_created']} insights, {activity_data['ml_predictions']} ML predictions\"\n        )\n        \n        # Website log\n        await self._log_to_website({\n            \"timestamp\": datetime.now().isoformat(),\n            \"level\": \"INFO\",\n            \"source\": \"DexBrain-Learning\",\n            \"message\": f\"Analyzed {activity_data['positions_analyzed']} positions, created {activity_data['insights_created']} insights\",\n            \"details\": {\n                \"cycle_duration\": f\"{activity_data['cycle_duration_seconds']:.1f}s\",\n                \"ml_predictions_made\": activity_data['ml_predictions'],\n                \"active_pools_tracked\": activity_data['active_pools'],\n                \"data_quality\": \"High\"\n            }\n        })\n    \n    async def _log_to_website(self, log_data: Dict[str, Any]):\n        \"\"\"\n        Log data to website in JSON format\n        \"\"\"\n        try:\n            log_entry = json.dumps(log_data)\n            website_logger.info(log_entry)\n        except Exception as e:\n            logger.warning(f\"Failed to log to website: {e}\")\n    \n    async def _update_session_stats(self):\n        \"\"\"\n        Update and log session statistics\n        \"\"\"\n        session_duration = datetime.now() - self.session_stats['session_start']\n        hours_running = session_duration.total_seconds() / 3600\n        \n        if hours_running > 0:\n            positions_per_hour = self.session_stats['positions_processed'] / hours_running\n            insights_per_hour = self.session_stats['insights_created'] / hours_running\n            \n            # Log stats every hour\n            if int(hours_running) > getattr(self, '_last_stats_hour', -1):\n                self._last_stats_hour = int(hours_running)\n                \n                await self._log_to_website({\n                    \"timestamp\": datetime.now().isoformat(),\n                    \"level\": \"INFO\",\n                    \"source\": \"DexBrain-Stats\",\n                    \"message\": f\"Session stats: {self.session_stats['positions_processed']} positions processed, {self.session_stats['insights_created']} insights created\",\n                    \"details\": {\n                        \"session_duration_hours\": f\"{hours_running:.1f}\",\n                        \"positions_per_hour\": f\"{positions_per_hour:.1f}\",\n                        \"insights_per_hour\": f\"{insights_per_hour:.1f}\",\n                        \"ml_predictions_made\": self.session_stats['ml_predictions_made'],\n                        \"training_sessions\": self.session_stats['training_sessions'],\n                        \"last_model_accuracy\": f\"{self.session_stats['last_training_accuracy']:.1%}\"\n                    }\n                })\n    \n    async def get_system_status(self) -> Dict[str, Any]:\n        \"\"\"\n        Get comprehensive system status\n        \"\"\"\n        training_status = await self.training_pipeline.get_training_status()\n        \n        return {\n            'system_status': 'running',\n            'session_stats': self.session_stats,\n            'training_status': training_status,\n            'active_pools': len(self.active_pools),\n            'learning_interval_minutes': self.fetch_interval / 60,\n            'quality_threshold': self.quality_threshold,\n            'ml_models_available': hasattr(self.training_pipeline, 'ml_optimizer') and self.training_pipeline.ml_optimizer is not None\n        }\n\n\nasync def main():\n    \"\"\"\n    Main function to run the enhanced continuous learning system\n    \"\"\"\n    # Configuration\n    API_KEY = \"c6f241c1dd5aea81977a63b2614af70d\"\n    SUBGRAPH_URL = \"https://gateway-arbitrum.network.thegraph.com/api/c6f241c1dd5aea81977a63b2614af70d/subgraphs/id/5zvR82QoaXYFyDEKLZ9t6v9adgnptxYpKpSbxtgVENFV\"\n    \n    # Initialize system\n    learning_system = EnhancedContinuousLearning(API_KEY, SUBGRAPH_URL)\n    \n    # Start continuous learning\n    await learning_system.start_continuous_learning()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())